---
tags:
  - Математика
  - Математика/Матстат
up: "[[Математическая статистика]]"
---

Условная энтропия $Y$ данного $X$ определяется как 
$$H(Y|X) = - \sum_{x \in \mathcal{X},y \in \mathcal{C}}p(x,y)\log\dfrac{p(x,y)}{p(x)}$$
где $\mathcal{X}$ and $\mathcal{Y}$ обозначают [опорные множества](https://translated.turbopages.org/proxy_u/en-ru.ru.b840aa42-6749e56c-2328b501-74722d776562/https/en.wikipedia.org/wiki/Support_(mathematics) "Поддержка (математика)") из $X$ и $Y$
Примечание Здесь принято считать, что выражение $0\log{0}$ следует рассматривать как равное нулю. Это потому, что $\lim_{ \theta \to 0^{+} }\theta \log \theta = 0$

Интуитивно обратите внимание, что по определению [ожидаемого значения](https://translated.turbopages.org/proxy_u/en-ru.ru.b840aa42-6749e56c-2328b501-74722d776562/https/en.wikipedia.org/wiki/Expected_value "Ожидаемое значение") и условная вероятность , $H(Y|X)$ может быть записано как $H(Y|X) = \mathbb{E}[f(X, Y)]$ где $f$ определяется как
$f(x,y) := - \log\left( \frac{p(x,y)}{p(x)} \right)=-\log(p(y|x))$. Можно думать, что $f$ каждая пара ассоциируется $(x,y)$ с величиной, измеряющей информационное содержание $(Y=y)$ данного $(X=x)$ Эта величина напрямую связана с объемом информации, необходимой для описания  $(Y=y)$ данного $(X=x)$ Следовательно, путем вычисления ожидаемого значения $f$ по всем парам значений $(x,y) \in \mathcal{X} \times \mathcal{Y}$ условная энтропия $H(Y|X)$ измеряет, сколько информации в среднем $X$ кодирует переменную $Y$