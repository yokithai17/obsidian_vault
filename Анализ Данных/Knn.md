---
tags:
  - Математика/ML
up: "[[Анализ Данных]]"
---

## Метод k-ближайших соседей (KNN)

Представим, что мы проводим классификацию объектов на два класса — красный или жёлтый. Нам дана некоторая обучающая выборка и целевой объект (серый):

![][https://yastatic.net/s3/education-portal/media/2_1_b7525e2dd2_5d0a4ba929.webp]

Мы хотим определить, к какому классу относится серый объект. Интуитивно очевидно, что он должен быть жёлтым, потому что все его соседи жёлтые. Эта интуиция и отражает суть метода KNN — классифицировать целевой объект, исходя из того, какие классы у объектов, которые максимально похожи на него.

Перейдём теперь к более формальному описанию алгоритма. Рассмотрим сначала задачу многоклассовой классификации, а регрессией займёмся позже.

Пусть дана обучающая выборка $X = (x_{i},y_{i})^{N}_{i=1},$ где $x_{i} \in \mathbb{X}, \ y_{i} \in \mathbb{Y} = 1, \dots ,C$ .Пусть также задана некоторая симметричная по своим аргументам функция расстояния $\rho : \mathbb{X} \times \mathbb{X} \rightarrow [ 0,+\inf)$ Предположим, что требуется классифицировать новый объект $u$. Для этого найдём $k$ наиболее близких к $u$ в смысле расстояния $\rho$ объектов обучающей выборки $X_{k}(u)=x^{(1)}_{u}, \dots ,x^{(k)}_{u}$​:

$$\forall x_{in} \in X_{k}(u) \ \forall x_{out} \in X \textbackslash X_{k}(u) \ \rho (u, x_{i}) \le \rho (u, x_{out})$$

Метку класса объекта $x_{u}^{(i)}$​ будем обозначать $y_{u}^{(i)}$​. Класс нового объекта тогда естественным образом определим как наиболее часто встречающийся класс среди объектов из $X_{k}​(u)$:

$$a(u)=\underset{y \in \mathbb{Y}}{argmax} \sum_{i=1}^{k} \mathbb{I}[y_{u}^{(i)}=y]$$

Формула может показаться страшной, но на самом деле всё довольно просто: для каждой метки класса $y \in \mathbb{Y}$ количество соседей $u$ с такой меткой можно посчитать, просто просуммировав по всем соседям индикаторы событий, соответствующих тому, что метка соседа равна $y$.

Легко заметить, что этот алгоритм позволяет также оценивать вероятности классов. Для этого достаточно просто посчитать частоты классов соседей:

$$\mathbb{P}(u \sim y)=\dfrac{\sum_{i=1}^{k} \mathbb{I}[y_{u}^{(i)}=y]}{k}$$​

Стоит, однако, понимать, что, хоть такая функция и удовлетворяет свойствам вероятности (она неотрицательна, аддитивна и ограничена единицей), это не более чем эвристика.

Несмотря на то что формально фаза обучения отсутствует, алгоритм может легко переобучиться. Вы можете убедиться в этом сами, использовав маленькое количество соседей (например, одного или двух), — границы классов оказываются довольно сложными. Происходит это из-за того, что параметрами алгоритма можно считать всю обучающую выборку, довольно большую по размеру. Из-за этого алгоритму легко подстроиться под конкретные данные.

### Выбор метрики


Может возникнуть закономерный вопрос, как же правильно выбрать функцию расстояния $\rho$. В подавляющем большинстве случаев обычное евклидово расстояние $\rho(x,y) = \sqrt{\sum_{i}(x_{i}, y_{i})^{2}}$ будет хорошим выбором. Однако в некоторых случаях другие функции будут подходить лучше, поэтому давайте разберём ещё несколько функций, наиболее используемых на практике.

![][https://yastatic.net/s3/education-portal/media/2_2_8440e10f60_6a35a4f1bc.webp]


#### Манхэттенская метрика

$$\rho(x,y)= \sum_{i}|x_{i}-y_{y}|$$
Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты, скорее всего, очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далёкими друг от друга. Этого недостатка лишена манхэттенская метрика — в ней вместо квадрата используется модуль.

#### Метрика Минковского

$$\rho(x,y) = \left(\sum_{i} \left| x_{i} - y_{i}\right|^{p} \right)^{\dfrac{1}{p}}$$

Является обобщением евклидовой (p=2p=2) и манхэттенской (p=1p=1) метрик.
#### Косинусное расстояние


$$\rho(x,y)=1-\cos(\alpha)=1-\dfrac{x \cdot y}{\left\| x\right\| \left\| y \right\|}$$Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно косинусное расстояние.

### Обобщения алгоритма

У оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной.

Давайте попробуем придумать, как исправить этот недостаток. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далёких. Можно заметить, что все индикаторы в формуле (2)(2) учитываются в сумме с одинаковыми коэффициентами. Возникает идея — назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу:

$$a(u)=\underset{y \in \mathbb{Y}}{argmax} \sum_{i=1}^{k} \omega_{i} \mathbb{I}[y_{u}^{(i)}=y]$$

Такой алгоритм называется **взвешенным KNN** (**weighted KNN**).

Есть множество вариантов выбора весов для объектов, которые можно поделить на две большие группы. В первой группе веса зависят лишь от порядкового номера объекта в отсортированном по близости к uu массиве $X_{k}(u)$. Чаще всего затухающие веса берутся линейно $\left(w_{i}=\dfrac{k+1−i}{k} \right)$ или экспоненциально $w_{i}​=q_{i}, 0<q<1)$ .
Однако здесь мы также не используем всю информацию, которая нам доступна. Зачем использовать порядок соседей, порождаемый расстояниями, если можно использовать сами расстояния?

Во второй группе методов вес — это некоторая функция от расстояния. Давайте подумаем, какие должны быть свойства у этой функции.

- Очевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями.
- Также необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далёких.

Таким образом вводится так называемая _ядерная функция_ (_kernel function_) $K : \mathbb{R} \rightarrow \mathbb{R}$ обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа:

$$a(u)=\underset{y \in \mathbb{Y}}{argmax} \sum_{i=1}^{k} 
K\left(
\dfrac{\rho(u,x_{u}^{(i)})}{h}
\right) \mathbb{I}[y_{u}^{(i)}=y]$$

где $h$ — некое положительное число, которое называется _шириной окна_.

От выбора ядра зависит гладкость аппроксимации, но на её качество этот выбор почти не влияет. Примеры ядерных функций в порядке увеличения их гладкости:

- $K(x)= \dfrac{1}{2} \mathbb{I}[|x| \le 1]$ — прямоугольное ядро;
- $K(x)=  (1-|x|) \cdot \mathbb{I}[|x| \le 1]$— треугольное ядро (непрерывное);
- $K(x)= 0.75(1-x^{2}) \mathbb{I}[|x| \le 1]$— ядро Епанечникова (гладкое везде, кроме –1 и 1);
- $K(x)=15/16(1-x^{2}) \mathbb{I}[|x| \le 1]$ — биквадратное ядро (гладкое везде);
- $K(x)= \dfrac{1}{\sqrt{2 \pi}} e^{-2x^{2}}$ — гауссовское ядро (бесконечно гладкое везде).
На практике чаще всего используют либо прямоугольное для простоты, либо гауссовское, в случае когда важна гладкость модели (немного забегая вперёд — это особенно важно в регрессии).

Ширина окна, в свою очередь, сильно влияет как раз на качество модели. При слишком маленькой ширине модель сильно подстраивается под обучающую выборку и теряет свою обобщающую способность. При слишком большой ширине, напротив, модель становится слишком простой. Универсальной ширины окна не существует, поэтому для каждой задачи её приходится подбирать отдельно.

#### Kernel regression

Алгоритм KNN можно довольно легко обобщить и на задачу регрессии. Самые очевидные способы — брать для некоторого ядра $K$ либо обычное среднее:

$$a(u)=\dfrac{1}{k}\sum_{i=1}^{k}y_{u}^{(i)},$$
либо взвешенный вариант:

$$a(u)=\dfrac{
\sum_{i=1}^{k}K(\dfrac{\rho(u,x_{u}^{(i)})}{h}y_{u}^{(i)})
}{
\sum_{i=1}^{k}K(\dfrac{\rho(u,x_{u}^{(i)})}{h}y_{u}^{(i)})
}$$

оследняя формула называется _формулой Надарая — Ватсона_. Она — один из непараметрических методов восстановления регрессии, объединённых названием _ядерная регрессия_ (_kernel regression_).

Выписать ответ, конечно, просто, но возникает интересный вопрос: можно ли использовать оптимизационные формулы из задачи классификации? Сначала давайте подумаем, что выдаст алгоритм, если формулу (4)(4) применить без изменений.

В задаче регрессии почти наверняка все значения $y_{u}^{(i)}$​ будут различными. Поэтому для любого $y$ сумма в формуле (4) будет состоять из не более чем одного слагаемого, а значит, максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм.

Давайте сперва подумаем, а для чего вообще в формуле (4)(4) используется индикатор. В задаче классификации индикатор — естественная мера близости двух объектов: если объекты совпадают, то значение 11, если различаются, то 00. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдаёт отличное от нуля значение лишь в одной точке $y=y_{i}$, — плохая мера близости.

В случае непрерывных значений yy естественно использовать более гладкие функции для выражения близости. Таким образом, для обобщения формулы (4)(4) на задачу регрессии нам необходимо всего лишь заменить индикатор на некоторую более гладкую функцию. При этом для действительных чисел чаще всего рассматривают не близость, а расстояние между ними, то есть некоторую метрику. Например, в качестве такой метрики можно взять квадрат евклидова расстояния $(y−y_{u}^{(i)})^{2}$. Отметим, что максимизация близости эквивалентна минимизации расстояния, и получим следующую формулу:

$$a(u)=\underset{y \in \mathbb{Y}}{argmin} \sum_{i=1}^{k} 
K\left(
\dfrac{\rho(u,x_{u}^{(i)})}{h}
\right) \left(y-y_{u}^{(i)}\right)^{2}$$

Выбор именно этой функции хорош тем, что у этой оптимизационной задачи есть точное решение, и оно записывается как раз формулой (6)(6).

Для ядерной регрессии справедливы те же рассуждения про выбор ядра и ширины окна, которые были приведены в прошлом разделе про классификацию.

Влияние ширины окна и вида ядра на вид функции:

![][https://yastatic.net/s3/education-portal/media/2_3_b71a476ace_8ba7ceac1d.webp]

![][https://yastatic.net/s3/education-portal/media/2_4_1f3e8fb8d7_4b5dfc48b1.webp]

![][https://yastatic.net/s3/education-portal/media/2_5_a94e8cc0dd_1d30dce7e4.webp]

### Преимущества и недостатки

Сперва поговорим о преимуществах алгоритма.

- Непараметрический, то есть не делает явных предположений о распределении данных.
- Очень простой в объяснении и интерпретации.
- Достаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy.
- Может быть использован как для классификации, так и для регрессии.

Несмотря на большие преимущества, алгоритм не лишён и минусов.

- Неэффективный по памяти, поскольку нужно хранить всю обучающую выборку.
- Вычислительно дорогой по той же причине.
- Чувствителен к масштабу данных, а также к неинформативным признакам.
- Для применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако, если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления.
### Применение

Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них:

- Рекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна.
- Поиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи.
- Поиск аномалий и выбросов. Из-за того что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел.
- Задача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи.

Вопрос сложности алгоритма неочевиден и требует детального анализа, который будет частично проведён в следующем разделе.

## Поиск ближайших соседей

Для того чтобы применять метод ближайших соседей, нужно уметь как-то находить этих самых соседей. С первого взгляда может показаться, что никакой проблемы нет: действительно, можно ведь просто перебрать все объекты из обучающей выборки $X=(x_{i},y_{i})^{N}_{i=1}$​, посчитать для каждого из них расстояние до тестового объекта и затем найти минимум.

Однако несмотря на то что сложность такого поиска линейная по $N$, она также зависит и от размерности пространства признаков. Если $x \in \mathbb{R}^{D}$, то сложность такого алгоритма поиска $O(ND)$. Если вспомнить, что в типичной задаче машинного обучения количество признаков $D$ может быть порядка 100, а размер выборки и вовсе может исчисляться десятками и сотнями тысяч объектов, то становится ясно, что такая сложность никуда не годится. Проблема осложняется ещё и тем, что данный поиск необходимо выполнять на этапе применения модели, который должен быть быстрым. Всё это означает, что возникает необходимость в более быстрых методах поиска ближайших соседей, чем простой перебор.

Все такие методы можно поделить на две основные группы: точные и приближённые. Последние, как следует из их названия, находят соседей лишь приближённо, то есть найденные объекты хоть и будут действительно близки, но не обязательно будут самыми близкими. В этом разделе мы подробнее рассмотрим методы из каждой группы.

Перед началом обзора стоит сказать, что хоть мы и рассматриваем алгоритмы поиска соседей именно в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в «Гугле» или «Яндексе» до всем известных алгоритмов «Ютьюба».

## Поиск ближайших соседей: точные методы

Точных методов существует довольно мало. Можно сказать, что их, по сути, два.

- Первый — полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие эвристики хоть и могут давать некоторый выигрыш по времени, но не улучшат асимптотическую сложность.
- Второй — k-d-деревья, о которых стоит поговорить подробнее.
### K-d-деревья

Представим на секунду, что у нас есть всего лишь один признак, то есть объекты выражаются вещественными числами, а не векторами. В этом случае для поиска ближайшего соседа напрашивается всем вам известное бинарное дерево поиска, которое позволяет находить элементы за логарифмическое время. Оказывается, существует аналог данной структуры в многомерном пространстве, который называется **k-d-дерево** (**k-d tree**, сокращение от k-dimensional tree).

Как и в обычном дереве поиска, в k-d-дереве каждый узел является объектом обучающей выборки, который особым образом делит пространство на два полупространства. Таким образом, всё пространство оказывается поделено на множество малых областей, и такое деление оказывается очень полезным при поиске ближайших соседей.

Рассмотрим подробнее, как строится такое дерево. Трудность в применении обычного дерева поиска состоит в том, что мы не можем напрямую сравнить два вектора так же, как два вещественных числа. Чтобы эту проблему преодолеть, узлы дерева будут делить пространство лишь по одной оси. При движении вниз по дереву оси, по которым точки делят пространство, циклически сменяют друг друга. Например, в двумерном пространстве корень будет отвечать за деление по x-координате, его сыновья — за деление по y-координате, а внуки — снова за x-координату, и т. д. Посмотрим, как это работает на примере:

![][https://yastatic.net/s3/education-portal/media/2_6_e762398494_7d10925856.webp]

На картинке выше корень $(30,40)$ делит все точки по оси х: слева оказываются точки, у которых $x \lt 30$, а справа — те, у которых $x \ge 30$. Аналогично левый сын корня $(5,25)$ делит своё поддерево по оси y: слева оказываются точки, у которых $y \lt 25$, а справа — те, у которых $y \ge 25$.

Остаётся вопрос — как выбирать точки, которые будут делить пространство пополам? Чтобы дерево было сбалансированным, нужно находить точку с медианой, соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближённому поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность.

Добавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d-деревьев, которые сохраняют сбалансированность при добавлении / удалении точек.

Поговорим теперь про то, как же находить ближайших соседей с помощью такого дерева. Будем производить обход дерева в глубину с двумя модификациями.

- Во-первых, будем запоминать наиболее близкую точку. Это позволит не заходить в поддеревья, задающие области, которые заведомо дальше, чем текущая наиболее близкая точка, поэтому не имеет смысла искать в них ближайших соседей.
- Во-вторых, будем прежде всего обходить те поддеревья, которые задают наиболее близкие области, а значит, с большей вероятностью содержат ближайшего соседа.
![][https://yastatic.net/s3/education-portal/media/2_7_2f24c2823e_0c7218aef4.gif]

Сложность метода по размеру обучающей выборки в среднем равна $O(\log N)$ при равномерном распределении точек. При большой размерности пространства, однако, алгоритму приходится посещать больше ветвей дерева, чтобы найти ближайших соседей. Например, если $N \sim D$, то сложность становится примерно такой же, как и в случае полного перебора. В общем случае считается, что для того чтобы асимптотика действительно была логарифмической, нужно, чтобы $N \underset{\sim}{\gt} 2^{D}$. Поэтому уже при количестве признаков порядка сотни алгоритм не даёт существенных преимуществ перед полным перебором.

Почитать по теме:

- [Хорошая презентация](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf), объясняющая структуру и поиск соседей.
- [Балансировка деревьев](https://en.wikipedia.org/wiki/K-d_tree#Balancing).